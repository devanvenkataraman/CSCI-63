deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
# print(table(classTrain))
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
# print(rfTmpTbl)
rferror <- (rfTmpTbl[2,1]+rfTmpTbl[1,2])/sum(rfTmpTbl)
print(paste("Random Forest error rate: ", rferror))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
# print(ldaTmpTbl)
ldaerror <- (ldaTmpTbl[2,1]+ldaTmpTbl[1,2])/sum(ldaTmpTbl)
print(paste("LDA error rate:", ldaerror))
dfTmp <- NULL
for ( kTmp in seq(1,20,length.out = 20) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate")
}
library(randomForest)
library(MASS)
library(class)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
# Generate the dataset, repeat 3 times
for (nObs in c(25, 100, 500)) {
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 3
# To modulate average difference between two classes' predictor values:
deltaClass <- 1
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
# print(table(classTrain))
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
# print(rfTmpTbl)
rferror <- (rfTmpTbl[2,1]+rfTmpTbl[1,2])/sum(rfTmpTbl)
print(paste("Random Forest error rate: ", rferror))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
# print(ldaTmpTbl)
ldaerror <- (ldaTmpTbl[2,1]+ldaTmpTbl[1,2])/sum(ldaTmpTbl)
print(paste("LDA error rate:", ldaerror))
dfTmp <- NULL
for ( kTmp in seq(1,nObs,length.out = 10) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate")
}
library(randomForest)
library(MASS)
library(class)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
# Generate the dataset, repeat 3 times
for (nObs in c(25, 100, 500)) {
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 3
# To modulate average difference between two classes' predictor values:
deltaClass <- 1
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
# print(table(classTrain))
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
# print(rfTmpTbl)
rferror <- (rfTmpTbl[2,1]+rfTmpTbl[1,2])/sum(rfTmpTbl)
print(paste("Random Forest error rate: ", rferror))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
# print(ldaTmpTbl)
ldaerror <- (ldaTmpTbl[2,1]+ldaTmpTbl[1,2])/sum(ldaTmpTbl)
print(paste("LDA error rate:", ldaerror))
dfTmp <- NULL
for ( kTmp in seq(1,nObs/2,length.out = 10) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate")
}
library(randomForest)
library(MASS)
library(class)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
# Generate the dataset, repeat 3 times
for (nObs in c(25, 100, 500)) {
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 3
# To modulate average difference between two classes' predictor values:
deltaClass <- 1
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
# print(table(classTrain))
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
# print(rfTmpTbl)
rferror <- (rfTmpTbl[2,1]+rfTmpTbl[1,2])/sum(rfTmpTbl)
print(paste("Random Forest error rate: ", rferror))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
# print(ldaTmpTbl)
ldaerror <- (ldaTmpTbl[2,1]+ldaTmpTbl[1,2])/sum(ldaTmpTbl)
print(paste("LDA error rate:", ldaerror))
dfTmp <- NULL
for ( kTmp in seq(1,nObs/2,length.out = 20) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate")
}
library(randomForest)
library(MASS)
library(class)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
# Generate the dataset, repeat 3 times
for (nObs in c(25, 100, 500)) {
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 3
# To modulate average difference between two classes' predictor values:
deltaClass <- 1
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
# print(table(classTrain))
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
# print(rfTmpTbl)
rferror <- (rfTmpTbl[2,1]+rfTmpTbl[1,2])/sum(rfTmpTbl)
print(paste("Random Forest error rate: ", rferror))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
# print(ldaTmpTbl)
ldaerror <- (ldaTmpTbl[2,1]+ldaTmpTbl[1,2])/sum(ldaTmpTbl)
print(paste("LDA error rate:", ldaerror))
dfTmp <- NULL
for ( kTmp in seq(1,nObs/2,length.out = 20) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
print(ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate"))
}
for (deltaClass in c(0.5, 1, 2)) {
# Generate the dataset, repeat 3 times
for (nObs in c(25, 100, 500)) {
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 3
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
# print(table(classTrain))
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
# print(rfTmpTbl)
rferror <- (rfTmpTbl[2,1]+rfTmpTbl[1,2])/sum(rfTmpTbl)
print(paste("Random Forest error rate: ", rferror))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
# print(ldaTmpTbl)
ldaerror <- (ldaTmpTbl[2,1]+ldaTmpTbl[1,2])/sum(ldaTmpTbl)
print(paste("LDA error rate:", ldaerror))
dfTmp <- NULL
for ( kTmp in seq(1,nObs/2,length.out = 20) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
print(ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate"))
}
}
library(randomForest)
library(MASS)
library(class)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
# Generate the dataset, repeat 3 times
for (nObs in c(25, 100, 500)) {
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 3
# To modulate average difference between two classes' predictor values:
deltaClass <- 1
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
dfTmp <- NULL
for ( kTmp in seq(1,nObs/2,length.out = 20) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
print(ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle(paste("KNN error rate with ", nObs, "observations"))
}
library(randomForest)
library(MASS)
library(class)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
# Generate the dataset, repeat 3 times
for (nObs in c(25, 100, 500)) {
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 3
# To modulate average difference between two classes' predictor values:
deltaClass <- 1
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
dfTmp <- NULL
for ( kTmp in seq(1,nObs/2,length.out = 20) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
print(ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle(paste("KNN error rate with ", nObs, "observations"))
}
library(randomForest)
library(MASS)
library(class)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
# Generate the dataset, repeat 3 times
for (nObs in c(25, 100, 500)) {
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 3
# To modulate average difference between two classes' predictor values:
deltaClass <- 1
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
dfTmp <- NULL
for ( kTmp in seq(1,nObs/2,length.out = 20) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
print(ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate"))
}
for (deltaClass in c(0.5, 1, 2)) {
# Generate the dataset, repeat 3 times
for (nObs in c(25, 100, 500)) {
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 3
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
# print(table(classTrain))
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
# print(rfTmpTbl)
rferror <- (rfTmpTbl[2,1]+rfTmpTbl[1,2])/sum(rfTmpTbl)
print(paste("Random Forest error rate: ", rferror))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
# print(ldaTmpTbl)
ldaerror <- (ldaTmpTbl[2,1]+ldaTmpTbl[1,2])/sum(ldaTmpTbl)
print(paste("LDA error rate:", ldaerror))
dfTmp <- NULL
for ( kTmp in seq(1,nObs/2,length.out = 20) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
print(ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate"))
}
}
for (deltaClass in c(0.5, 1, 2)) {
# Generate the dataset, repeat 3 times
for (nObs in c(100, 500)) {
# How many predictors are associated with outcome:
nClassVars <- 2
# How many predictors are not:
nNoiseVars <- 3
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
# print(table(classTrain))
print(paste("Number of observations: ", nObs))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
# print(rfTmpTbl)
rferror <- (rfTmpTbl[2,1]+rfTmpTbl[1,2])/sum(rfTmpTbl)
print(paste("Random Forest error rate: ", rferror))
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
# print(ldaTmpTbl)
ldaerror <- (ldaTmpTbl[2,1]+ldaTmpTbl[1,2])/sum(ldaTmpTbl)
print(paste("LDA error rate:", ldaerror))
dfTmp <- NULL
for ( kTmp in seq(1,nObs/2,length.out = 20) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
print(ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate"))
}
}
for (nClassVars in c(2,5) {
for (nClassVars in c(2,5)) {
# Generate the dataset, repeat 3 times
for (nNoiseVars in c(1, 3, 10)) {
# How many predictors are associated with outcome:
deltaClass <- 1
nObs <- 500
# Simulate training and test datasets with an interaction
# between attribute levels associated with the outcome:
xyzTrain <- matrix(rnorm(nObs*(nClassVars+nNoiseVars)),nrow=nObs,ncol=nClassVars+nNoiseVars)
xyzTest <- matrix(rnorm(10000*(nClassVars+nNoiseVars)),nrow=10000,ncol=nClassVars+nNoiseVars)
classTrain <- 1
classTest <- 1
for ( iTmp in 1:nClassVars ) {
deltaTrain <- sample(deltaClass*c(-1,1),nObs,replace=TRUE)
xyzTrain[,iTmp] <- xyzTrain[,iTmp] + deltaTrain
classTrain <- classTrain * deltaTrain
deltaTest <- sample(deltaClass*c(-1,1),10000,replace=TRUE)
xyzTest[,iTmp] <- xyzTest[,iTmp] + deltaTest
classTest <- classTest * deltaTest
}
classTrain <- factor(classTrain > 0)
print(paste("Number of predictor variables: ", nClassVars))
print(paste("Number of noise variables: ", nNoiseVars))
rfRes <- randomForest(xyzTrain,classTrain)
rfTmpTbl <- table(classTest,predict(rfRes,newdata=xyzTest))
rferror <- (rfTmpTbl[2,1]+rfTmpTbl[1,2])/sum(rfTmpTbl)
ldaRes <- lda(xyzTrain,classTrain)
ldaTmpTbl <- table(classTest,predict(ldaRes,newdata=xyzTest)$class)
ldaerror <- (ldaTmpTbl[2,1]+ldaTmpTbl[1,2])/sum(ldaTmpTbl)
dfTmp <- NULL
for ( kTmp in seq(1,nObs/2,length.out = 20) ) {
knnRes <- knn(xyzTrain,xyzTest,classTrain,k=kTmp)
tmpTbl <- table(classTest,knnRes)
dfTmp <- rbind(dfTmp,data.frame(err=1-sum(diag(tmpTbl))/sum(tmpTbl),k=kTmp))
}
print(ggplot(dfTmp,aes(x=k,y=err))+geom_point()+scale_x_log10()+geom_hline(aes(yintercept = err,colour=type),data=data.frame(type=c("LDA","RF"),err=c(1-sum(diag(ldaTmpTbl))/sum(ldaTmpTbl),1-sum(diag(rfTmpTbl))/sum(rfTmpTbl))))+ggtitle("KNN error rate"))
}
}
?randomForest
