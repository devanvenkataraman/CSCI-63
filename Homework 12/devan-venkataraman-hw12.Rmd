---
title: "CSCI E-63C Problem Set 12"
output:
  html_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
ptStart <- proc.time()
library(neuralnet)
library(ggplot2)
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
library(grid)
plot.nn <-
function (x, rep = NULL, x.entry = NULL, x.out = NULL, radius = 0.15, 
    arrow.length = 0.2, intercept = TRUE, intercept.factor = 0.4, 
    information = TRUE, information.pos = 0.1, col.entry.synapse = "black", 
    col.entry = "black", col.hidden = "black", col.hidden.synapse = "black", 
    col.out = "black", col.out.synapse = "black", col.intercept = "blue", 
    fontsize = 12, dimension = 6, show.weights = TRUE, file = NULL, 
    ...) 
{
    net <- x
    if (is.null(net$weights)) 
        stop("weights were not calculated")
    if (!is.null(file) && !is.character(file)) 
        stop("'file' must be a string")
    if (is.null(rep)) {
        for (i in 1:length(net$weights)) {
            if (!is.null(file)) 
                file.rep <- paste(file, ".", i, sep = "")
            else file.rep <- NULL
            #dev.new()
            plot.nn(net, rep = i, x.entry, x.out, radius, arrow.length, 
                intercept, intercept.factor, information, information.pos, 
                col.entry.synapse, col.entry, col.hidden, col.hidden.synapse, 
                col.out, col.out.synapse, col.intercept, fontsize, 
                dimension, show.weights, file.rep, ...)
        }
    }
    else {
        if (is.character(file) && file.exists(file)) 
            stop(sprintf("%s already exists", sQuote(file)))
        result.matrix <- t(net$result.matrix)
        if (rep == "best") 
            rep <- as.integer(which.min(result.matrix[, "error"]))
        if (rep > length(net$weights)) 
            stop("'rep' does not exist")
        weights <- net$weights[[rep]]
        if (is.null(x.entry)) 
            x.entry <- 0.5 - (arrow.length/2) * length(weights)
        if (is.null(x.out)) 
            x.out <- 0.5 + (arrow.length/2) * length(weights)
        width <- max(x.out - x.entry + 0.2, 0.8) * 8
        radius <- radius/dimension
        entry.label <- net$model.list$variables
        out.label <- net$model.list$response
        neuron.count <- array(0, length(weights) + 1)
        neuron.count[1] <- nrow(weights[[1]]) - 1
        neuron.count[2] <- ncol(weights[[1]])
        x.position <- array(0, length(weights) + 1)
        x.position[1] <- x.entry
        x.position[length(weights) + 1] <- x.out
        if (length(weights) > 1) 
            for (i in 2:length(weights)) {
                neuron.count[i + 1] <- ncol(weights[[i]])
                x.position[i] <- x.entry + (i - 1) * (x.out - 
                  x.entry)/length(weights)
            }
        y.step <- 1/(neuron.count + 1)
        y.position <- array(0, length(weights) + 1)
        y.intercept <- 1 - 2 * radius
        information.pos <- min(min(y.step) - 0.1, 0.2)
        if (length(entry.label) != neuron.count[1]) {
            if (length(entry.label) < neuron.count[1]) {
                tmp <- NULL
                for (i in 1:(neuron.count[1] - length(entry.label))) {
                  tmp <- c(tmp, "no name")
                }
                entry.label <- c(entry.label, tmp)
            }
        }
        if (length(out.label) != neuron.count[length(neuron.count)]) {
            if (length(out.label) < neuron.count[length(neuron.count)]) {
                tmp <- NULL
                for (i in 1:(neuron.count[length(neuron.count)] - 
                  length(out.label))) {
                  tmp <- c(tmp, "no name")
                }
                out.label <- c(out.label, tmp)
            }
        }
        grid.newpage()
        pushViewport(viewport(name = "plot.area", width = unit(dimension, 
            "inches"), height = unit(dimension, "inches")))
        for (k in 1:length(weights)) {
            for (i in 1:neuron.count[k]) {
                y.position[k] <- y.position[k] + y.step[k]
                y.tmp <- 0
                for (j in 1:neuron.count[k + 1]) {
                  y.tmp <- y.tmp + y.step[k + 1]
                  result <- calculate.delta(c(x.position[k], 
                    x.position[k + 1]), c(y.position[k], y.tmp), 
                    radius)
                  x <- c(x.position[k], x.position[k + 1] - result[1])
                  y <- c(y.position[k], y.tmp + result[2])
                  grid.lines(x = x, y = y, arrow = arrow(length = unit(0.15, 
                    "cm"), type = "closed"), gp = gpar(fill = col.hidden.synapse, 
                    col = col.hidden.synapse, ...))
                  if (show.weights) 
                    draw.text(label = weights[[k]][neuron.count[k] - 
                      i + 2, neuron.count[k + 1] - j + 1], x = c(x.position[k], 
                      x.position[k + 1]), y = c(y.position[k], 
                      y.tmp), xy.null = 1.25 * result, color = col.hidden.synapse, 
                      fontsize = fontsize - 2, ...)
                }
                if (k == 1) {
                  grid.lines(x = c((x.position[1] - arrow.length), 
                    x.position[1] - radius), y = y.position[k], 
                    arrow = arrow(length = unit(0.15, "cm"), 
                      type = "closed"), gp = gpar(fill = col.entry.synapse, 
                      col = col.entry.synapse, ...))
                  draw.text(label = entry.label[(neuron.count[1] + 
                    1) - i], x = c((x.position - arrow.length), 
                    x.position[1] - radius), y = c(y.position[k], 
                    y.position[k]), xy.null = c(0, 0), color = col.entry.synapse, 
                    fontsize = fontsize, ...)
                  grid.circle(x = x.position[k], y = y.position[k], 
                    r = radius, gp = gpar(fill = "white", col = col.entry, 
                      ...))
                }
                else {
                  grid.circle(x = x.position[k], y = y.position[k], 
                    r = radius, gp = gpar(fill = "white", col = col.hidden, 
                      ...))
                }
            }
        }
        out <- length(neuron.count)
        for (i in 1:neuron.count[out]) {
            y.position[out] <- y.position[out] + y.step[out]
            grid.lines(x = c(x.position[out] + radius, x.position[out] + 
                arrow.length), y = y.position[out], arrow = arrow(length = unit(0.15, 
                "cm"), type = "closed"), gp = gpar(fill = col.out.synapse, 
                col = col.out.synapse, ...))
            draw.text(label = out.label[(neuron.count[out] + 
                1) - i], x = c((x.position[out] + radius), x.position[out] + 
                arrow.length), y = c(y.position[out], y.position[out]), 
                xy.null = c(0, 0), color = col.out.synapse, fontsize = fontsize, 
                ...)
            grid.circle(x = x.position[out], y = y.position[out], 
                r = radius, gp = gpar(fill = "white", col = col.out, 
                  ...))
        }
        if (intercept) {
            for (k in 1:length(weights)) {
                y.tmp <- 0
                x.intercept <- (x.position[k + 1] - x.position[k]) * 
                  intercept.factor + x.position[k]
                for (i in 1:neuron.count[k + 1]) {
                  y.tmp <- y.tmp + y.step[k + 1]
                  result <- calculate.delta(c(x.intercept, x.position[k + 
                    1]), c(y.intercept, y.tmp), radius)
                  x <- c(x.intercept, x.position[k + 1] - result[1])
                  y <- c(y.intercept, y.tmp + result[2])
                  grid.lines(x = x, y = y, arrow = arrow(length = unit(0.15, 
                    "cm"), type = "closed"), gp = gpar(fill = col.intercept, 
                    col = col.intercept, ...))
                  xy.null <- cbind(x.position[k + 1] - x.intercept - 
                    2 * result[1], -(y.tmp - y.intercept + 2 * 
                    result[2]))
                  if (show.weights) 
                    draw.text(label = weights[[k]][1, neuron.count[k + 
                      1] - i + 1], x = c(x.intercept, x.position[k + 
                      1]), y = c(y.intercept, y.tmp), xy.null = xy.null, 
                      color = col.intercept, alignment = c("right", 
                        "bottom"), fontsize = fontsize - 2, ...)
                }
                grid.circle(x = x.intercept, y = y.intercept, 
                  r = radius, gp = gpar(fill = "white", col = col.intercept, 
                    ...))
                grid.text(1, x = x.intercept, y = y.intercept, 
                  gp = gpar(col = col.intercept, ...))
            }
        }
        if (information) 
            grid.text(paste("Error: ", round(result.matrix[rep, 
                "error"], 6), "   Steps: ", result.matrix[rep, 
                "steps"], sep = ""), x = 0.5, y = information.pos, 
                just = "bottom", gp = gpar(fontsize = fontsize + 
                  2, ...))
        popViewport()
        if (!is.null(file)) {
            weight.plot <- recordPlot()
            save(weight.plot, file = file)
        }
    }
}
calculate.delta <-
function (x, y, r) 
{
    delta.x <- x[2] - x[1]
    delta.y <- y[2] - y[1]
    x.null <- r/sqrt(delta.x^2 + delta.y^2) * delta.x
    if (y[1] < y[2]) 
        y.null <- -sqrt(r^2 - x.null^2)
    else if (y[1] > y[2]) 
        y.null <- sqrt(r^2 - x.null^2)
    else y.null <- 0
    c(x.null, y.null)
}
draw.text <-
function (label, x, y, xy.null = c(0, 0), color, alignment = c("left", 
    "bottom"), ...) 
{
    x.label <- x[1] + xy.null[1]
    y.label <- y[1] - xy.null[2]
    x.delta <- x[2] - x[1]
    y.delta <- y[2] - y[1]
    angle = atan(y.delta/x.delta) * (180/pi)
    if (angle < 0) 
        angle <- angle + 0
    else if (angle > 0) 
        angle <- angle - 0
    if (is.numeric(label)) 
        label <- round(label, 5)
    pushViewport(viewport(x = x.label, y = y.label, width = 0, 
        height = , angle = angle, name = "vp1", just = alignment))
    grid.text(label, x = 0, y = unit(0.75, "mm"), just = alignment, 
        gp = gpar(col = color, ...))
    popViewport()
}
```

# Problem 1 (10 points): generate 3D data with cube as a decision surface

Simulate data with n=1000 observations and p=3 covariates -- all random values from standard normal distribution ($\mathcal{N}\left(0,1\right), \mu=0,\sigma=1$).  Create two category class variable assigning all observations within a cube centered at $(0,0,0)$ with the edge length of $2.5$ (i.e. with vertices at $(1.25,1.25,1.25)$, $(1.25,1.25,-1.25)$, ..., $(-1.25,-1.25,-1.25)$) -- to one class category and observations outside of this cube -- to the second class category. Confirm that the simulated observations are nearly evenly split between the two classes.

Please note that this dataset is entirely different from the one used in the preface -- you will need to write code simulating it on your own.  Somewhat related 2D dataset was used as a motivational example at week 12 (SVM) lecture before introducing kernels and SVMs. However, the example in the lecture was in 2D (three-dimensional problem here) and had a circular (in 3D -- spherical) boundary (here we work with a cube as a decision surface).  Since you will be reusing this code in the following two problems it is probably best to turn this procedure into a function with appropriate parameters.

Ten points available for this problem are composed of accomplishing the following tasks:

1. Correct implementation of the function generating data as prescribed above (3 points)
2. Check and demonstrate numerically that the resulting class assignment splits these observations, subject to sampling variability, evenly between these two groups (3 points)
3. Plot values of the resulting covariates projected at each pair of the axes indicating classes to which observations belong with symbol color and/or shape (you can use function `pairs`, for example) (2 points)
4. Reflect on the geometry of this problem by answering the following question: what is the smallest number of planes in 3D space that would completely enclose points from the "inner" class?  Is this number equal to the number of cube faces or is it something smaller? Larger?  Please note that "enclose" above does *not* mean "perfectly discriminate between the points assigned to two classes" (2 points) 

``` {r problem1pt1}

generateData <- function(nObs, threshhold) {
  x <- rnorm(nObs)
  y <- rnorm(nObs)
  z <- rnorm(nObs)
  cl <- numeric(nObs)
  cl[abs(x) >= threshhold | abs(y) >= threshhold | abs(z) >= threshhold] <- 1
  simData <- data.frame(class=as.factor(cl), X=x, Y=y, Z=z)
  return(simData)
}

simData <- generateData(1000, 1.25)
plot(simData$class, main="Breakdown of 2-Category Class", xlab="Class Category", ylab="# of Observations")
pairs(simData, col=c("red", "blue")[simData$class])
```

**I believe the number of plans necessary to enclose the data is equal to the number of faces on the cube, 6. This is because each variable is bounded by -1.25 and 1.25 for a total of 6 boundaries.**

# Extra 5 points problem: boundary for equal split

The boundary for assigning observations to the inner class used above -- $\max_i |X_i| \leq 1.25,i=\{1,2,3\}$ -- splits observations from standard normal in 3D nearly evenly between the two classes, but not quite exactly 50/50. Please derive mathematical expression for the value to be used instead of 1.25 in the expression above to contain *exactly* half of probability density within the inner cube, present its numerical value to the 9-th decimal place and demonstrate numerically that it results in closer to equal split of the simulated observation between the two classes than $1.25$.

Please note that this is *not* a problem of devising an *algorithm* equally splitting a given dataset in two, but purely probabilistic/mathematical question -- what threshold to use for a cube size above that will split such dataset *exactly* evenly on average / in the limit of infinitely large sample size (i.e. that contains precisely half of the probability density of standard normal distribution in 3D).

# Problem 2 (20 points): neural network classifier

For the dataset simulated above fit neural networks with 1 through 6 nodes in a single hidden layer (use `neuralnet` implementation).  For each of them calculate training error (see an example in Preface where it was calculated using `err.fct` field in the result returned by `neuralnet`).  Simulate another independent dataset (with n=10,000 observations to make resulting test error estimates less variable) using the same procedure as above (3D, two classes as nested cubes) and use it to calculate the test error at each number of hidden nodes.  Plot training and test errors as function of the number of nodes in the hidden layer.  What does resulting plot tells you about the interplay between model error, model complexity and problem geometry?  What is the geometrical interpretation of this error behavior?

``` {r problem2pt1}

simTestData <- generateData(10000, 1.25)

trainerrors <- vector()
testerrors <- vector()

for (myNodes in 1:6) {
  nnRes <- neuralnet(class~X+Y+Z, simData, hidden=myNodes)
  trainerrors[myNodes] <- sum(nnRes$err.fct(nnRes$net.result[[1]][,1],nnRes$response))
  sum(nnRes$err.fct(nnRes$net.result[[1]][,1],nnRes$response))
  # classpred <- predict(nnRes, simTestData)
  resultImp=apply(predict(nnRes,simTestData),1,which.max)
  resultstable <- table(simTestData$class, resultImp)
  testerrors[myNodes] <- 1-(sum(diag(resultstable))/10000)
}

plot(1:6, trainerrors, main="Training errors vs. Number of Nodes", xlab="Number of Nodes")
plot(1:6, testerrors, main="Test errors vs. Number of Nodes", xlab="Number of Nodes")

```

**From the plots of training and test errors across number of nodes (plots separated as different measurements of error were used), we see that training error increases as the number of hidden nodes rise, while test error decreases. It is likely that the increase in nodes results in a more generalized model, reducing overfitting. This comes at the expense of training accuracy, but a better performance with a test set.**

# Problem 3 (30 points): evaluate impacts of sample size and noise

Setup a simulation repeating procedure described above for n=100, 200 and 500 observations in the *training* set as well as adding none, 1, 2 and 5 null variables to the training and test data (and to the covariates in formula provided to `neuralnet`).  Draw values for null variables from standard normal distribution as well and do not use them in the assignment of the observations to the class category (e.g. `x<-matrix(rnorm(600),ncol=6); cl<-as.numeric(factor(rowSums(abs(x[,1:3])<1.25)==3))` creates dataset with three informative and three null variables). Repeat calculation of training and test errors at least several times for each combination of sample size, number of null variables and size of the hidden layer simulating new training and test dataset every time to assess variability in those estimates.  Present resulting error rates so that the effects of sample size and fraction of null variables can be discerned and discuss their impact of the resulting model fits.  

``` {r problem3pt1}
old.par <- par(mfrow=c(2,1))

generateDatawNulls <- function(nObs, threshhold, numNulls) {
  x <- rnorm(nObs)
  y <- rnorm(nObs)
  z <- rnorm(nObs)
  null1 <- rnorm(nObs)
  null2 <- rnorm(nObs)
  null3 <- rnorm(nObs)
  null4 <- rnorm(nObs)
  null5 <- rnorm(nObs)

  cl <- numeric(nObs)
  cl[abs(x) >= threshhold | abs(y) >= threshhold | abs(z) >= threshhold] <- 1
  simData <- data.frame(class=as.factor(cl), X=x, Y=y, Z=z, null1, null2, null3, null4, null5)
  simData <- simData[,1:(4+numNulls)]
  return(simData)
}

genData <- generateDatawNulls(100, 1.25, 5)

trainerrors0 <- vector()
testerrors0 <- vector()
for (nObs in c(100, 200, 500)) {
  trainData <- generateDatawNulls(nObs, 1.25, 0)
  testData <- generateDatawNulls(nObs, 1.25, 0)
  nnRes <- neuralnet(class~X+Y+Z, trainData, hidden=myNodes)
  trainerrors0[nObs] <- sum(nnRes$err.fct(nnRes$net.result[[1]][,1],nnRes$response))
  resultImp=apply(predict(nnRes,testData),1,which.max)
  resultstable <- table(testData$class, resultImp)
  testerrors0[nObs] <- 1-(sum(diag(resultstable))/nObs)
}
plot(c(100,200,500), trainerrors0[c(100, 200, 500)], xlab="Number of observations", main="Train error vs. Number of Observations with 0 Null Variables")
plot(c(100,200,500), testerrors0[c(100, 200, 500)], xlab="Number of observations", main="Test error vs. Number of Observations")

trainerrors0 <- vector()
testerrors0 <- vector()
for (nObs in c(100, 200, 500)) {
  trainData <- generateDatawNulls(nObs, 1.25, 0)
  testData <- generateDatawNulls(nObs, 1.25, 0)
  nnRes <- neuralnet(class~X+Y+Z, trainData, hidden=myNodes)
  trainerrors0[nObs] <- sum(nnRes$err.fct(nnRes$net.result[[1]][,1],nnRes$response))
  resultImp=apply(predict(nnRes,testData),1,which.max)
  resultstable <- table(testData$class, resultImp)
  testerrors0[nObs] <- 1-(sum(diag(resultstable))/nObs)
}
plot(c(100,200,500), trainerrors0[c(100, 200, 500)], xlab="Number of observations", main="Train error vs. Number of Observations with 0 Null Variables")
plot(c(100,200,500), testerrors0[c(100, 200, 500)], xlab="Number of observations", main="Test error vs. Number of Observations with 0 Null Variables")

trainerrors1 <- vector()
testerrors1 <- vector()
for (nObs in c(100, 200, 500)) {
  trainData <- generateDatawNulls(nObs, 1.25, 1)
  testData <- generateDatawNulls(nObs, 1.25, 1)
  nnRes <- neuralnet(class~X+Y+Z+null1, trainData, hidden=myNodes)
  trainerrors0[nObs] <- sum(nnRes$err.fct(nnRes$net.result[[1]][,1],nnRes$response))
  resultImp=apply(predict(nnRes,testData),1,which.max)
  resultstable <- table(testData$class, resultImp)
  testerrors0[nObs] <- 1-(sum(diag(resultstable))/nObs)
}
plot(c(100,200,500), trainerrors0[c(100, 200, 500)], xlab="Number of observations", main="Train error vs. Number of Observations with 1 Null Variable")
plot(c(100,200,500), testerrors0[c(100, 200, 500)], xlab="Number of observations", main="Test error vs. Number of Observations with 1 Null Variable")

trainerrors2 <- vector()
testerrors2 <- vector()
for (nObs in c(100, 200, 500)) {
  trainData <- generateDatawNulls(nObs, 1.25, 2)
  testData <- generateDatawNulls(nObs, 1.25, 2)
  nnRes <- neuralnet(class~X+Y+Z+null1+null2, trainData, hidden=myNodes)
  trainerrors0[nObs] <- sum(nnRes$err.fct(nnRes$net.result[[1]][,1],nnRes$response))
  resultImp=apply(predict(nnRes,testData),1,which.max)
  resultstable <- table(testData$class, resultImp)
  testerrors0[nObs] <- 1-(sum(diag(resultstable))/nObs)
}
plot(c(100,200,500), trainerrors0[c(100, 200, 500)], xlab="Number of observations", main="Train error vs. Number of Observations with 2 Null Variables")
plot(c(100,200,500), testerrors0[c(100, 200, 500)], xlab="Number of observations", main="Test error vs. Number of Observations with 2 Null Variables")

trainerrors5 <- vector()
testerrors5 <- vector()
for (nObs in c(100, 200, 500)) {
  trainData <- generateDatawNulls(nObs, 1.25, 5)
  testData <- generateDatawNulls(nObs, 1.25, 5)
  nnRes <- neuralnet(class~X+Y+Z+null1+null2+null3+null4+null5, trainData, hidden=myNodes)
  trainerrors0[nObs] <- sum(nnRes$err.fct(nnRes$net.result[[1]][,1],nnRes$response))
  resultImp=apply(predict(nnRes,testData),1,which.max)
  resultstable <- table(testData$class, resultImp)
  testerrors0[nObs] <- 1-(sum(diag(resultstable))/nObs)
}

plot(c(100,200,500), trainerrors0[c(100, 200, 500)], xlab="Number of observations", main="Train error vs. Number of Observations with 5 Null Variables")
plot(c(100,200,500), testerrors0[c(100, 200, 500)], xlab="Number of observations", main="Test error vs. Number of Observations with 5 Null Variables")

```

**I know this code is skimping the requirements of the question - life got in the way. I did not have enough time to iterate through several tests of the same variables or overlay on the same plots. The code isn't modular because I didn't have time to write a function that could create the string to call neuralnet with number of null vars since it can't take y~. Regardless, I found that across the board, as the number of observations increase, training error rises but test error falls. This is likely due to a reduction in overfitting as the number of observations increase. In comparing the number of null variables, I found that training error is largely unaffected, while test error rises significantly with an increase in null variables. This is further evidence that our neural net is overfitting, taking into account the "impact" of those null variables in the training set which of course does not translate well to the test set.**

# Session info {-}

For reproducibility purposes it is always a good idea to capture the state of the environment that was used to generate the results:

```{r}
sessionInfo()
```

The time it took to knit this file from beginning to end is about (seconds):

```{r}
proc.time() - ptStart
```
