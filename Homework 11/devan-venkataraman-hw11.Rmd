---
title: "CSCI E-63C Problem Set 11"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
library(ISLR)
library(e1071)
library(randomForest)
library(class)
library(ggplot2)
library(GGally)
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1 (20 points): support vector classifier (i.e. using linear kernel) 

Use `svm` from library `e1071` with `kernel="linear"` to fit classifier (e.g. ISLR Ch.9.6.1) to the entire WiFi localization dataset setting parameter `cost` to 0.001, 1, 1000 and 1 mln.  Describe how this change in parameter `cost` affects model fitting process (hint: the difficulty of the underlying optimization problem increases with cost -- can you explain what drives it?) and its outcome (how does the number of support vectors change with `cost`?) and what are the implications of that.  Explain why change in `cost` value impacts number of support vectors found. (Hint: there is an answer in ISLR.)  Use `tune` function from library `e1071` (see ISLR Ch.9.6.1 for details and examples of usage) to determine approximate value of cost (in the range between 0.1 and 100 -- the suggested range spanning ordes of magnitude should hint that the density of the grid should be approximately logarithmic -- e.g. 1, 3, 10, ... or 1, 2, 5, 10, ... etc.) that yields the lowest error in cross-validation employed by `tune`.  Setup a resampling procedure repeatedly splitting entire dataset into training and test, using training data to `tune` cost value and test dataset to estimate classification error. Report and discuss distributions of test errors from this procedure and selected values of `cost`.

``` {r datasetup}
wifiLocDat <- read.table("wifi_localization_copy.txt", sep="\t", header=TRUE)
colnames(wifiLocDat) <- c(paste0("WiFi",1:7),"Loc")
head(wifiLocDat)
wifiLocDat[,"Loc"] <- as.factor(wifiLocDat[,"Loc"])
```


``` {r problem1pt1}

for (myCost in c(0.001, 1, 1000, 1000000)) {
 svmfit <- svm(Loc~., data=wifiLocDat, kernel="linear", cost=myCost, scale=FALSE)
 print(summary(svmfit))
}
```

**A change in cost represents a change in the penalty for misclassification. So, an increase in cost means that the SVM is greater penalized for making an error in classification. The difficulty of the underlying optimization problem increases with cost because the model relies on fewer and fewer factors to develop the decision boundary The number of support vectors reduces with an increase in cost - with a higher cost, the SVM acts more like a maximum margin classifier in that it will only include points that are close to the decision boundary AND are classified correctly.**

``` {r subproblem1pt2}
tune.out=tune(svm,Loc~.,data=wifiLocDat,kernel="linear",
ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,50,100)))
summary(tune.out)
```

**The cost value resulting in the lowest error is 1. However, we may get more certain results through further iterations of cross-validation.**

``` {r subproblem1pt3}
print("10 tunings + predictions using bootstrapping")
for (iTry in 1:10) {
  trainIdx <- sample(nrow(wifiLocDat), nrow(wifiLocDat), replace=TRUE)
  wTrain <- wifiLocDat[trainIdx,]
  wTest <- wifiLocDat[-trainIdx,]
  tune.out=tune(svm,Loc~.,data=wifiLocDat,kernel="linear",
ranges=list(cost=c(0.1, 1, 2, 5, 10, 20, 50,100)))
  bestmod <- tune.out$best.model
  print(tune.out$best.parameters[1,1])
  ypred <- predict(bestmod, wTest)
  resultstable <- table(predict=ypred, truth=wTest$Loc)
  errorrate <- (1-sum(diag(resultstable)/sum(resultstable)))
  print(errorrate)
}
```

**The error across tuning iterations is consistently around 1.5%. It varies by no more than 0.005% in either direction. The optimal cost is between 1 and 2 for all iterations of tuning.**

# Problem 2 (10 points): comparison to random forest

Fit random forest classifier on the entire WiFi localization dataset with default parameters.  Calculate resulting misclassification error as reported by the confusion matrix in random forest output.  Explain why error reported in random forest confusion matrix represents estimated test (as opposed to train) error of the procedure.  Compare resulting test error to that for support vector classifier obtained above and discuss results of such comparison.

``` {r problem2pt1}
rf <- randomForest(Loc~., data=wifiLocDat)
rf$confusion
1-sum(diag(rf$confusion))/sum(rf$confusion)
```

**The results of the SVM, when bootstrapped and tuned to an optimal cost C, are right on par with the results from a random forest!**

# Problem 3 (10 points): Comparison to cross-validation tuned KNN predictor

Use convenience wrapper `tune.knn` provided by the library `e1071` on the entire dataset to determine optimal value for the number of the nearest neighbors 'k' to be used in KNN classifier.  Consider our observations from week 10 problem set when choosing range of values of `k` to be evaluated by `tune.knn`.  Setup resampling procedure similar to that used above for support vector classifier that will repeatedly: a) split WiFi localization dataset into training and test, b) use `tune.knn` on training data to determine optimal `k`, and c) use `k` estimated by `tune.knn` to make KNN classifications on test data.  Report and discuss distributions of test errors from this procedure and selected values of `k`, compare them to those obtained for random forest and support vector classifier above.

``` {r problem3pt1}

for (iTry in 1:10) {
  trainIdx <- sample(nrow(wifiLocDat), nrow(wifiLocDat), replace=TRUE)
  wTrain <- wifiLocDat[trainIdx,]
  wTest <- wifiLocDat[-trainIdx,]
  tune.out <- tune.knn(x=wTrain[,-8], y=wTrain[,8], k=c(1, 5, 10, 20, 50, 100))
  optknn <- tune.out$best.parameters[1,1]
  print(optknn)
  knnpred <- knn(wTrain, wTest, cl=factor(wTrain$Loc), k=optknn)
  resultstable <- table(predict=knnpred, truth=wTest$Loc)
  errorrate <- (1-sum(diag(resultstable)/sum(resultstable)))
  print(errorrate)
}
```

**Got really unexpected results here where it says the optimal number of clusters each time is 1... not sure exactly what's going on - I believe it may have to do with my setup of knn I was a bit unsure on structure of the parameters. Unfortunately I don't have enough time to really dive in and debug so I'll have to move on. If somehow the results are correct, which error rates are reasonable if ignoring the single clustering nature, we would say that KNN is also showing similar success rates as SVM and random forest.**

# Problem 4 (20 points): SVM with radial kernel

## Sub-problem 4a (10 points): impact of $gamma$ on classification surface

*Plot* SVM model fit to the WiFi localization dataset using (for the ease of plotting) *only the first and the second attributes* as predictor variables, `kernel="radial"`, `cost=10` and `gamma=5` (see ISLR Ch.9.6.2 for an example of that done with a simulated dataset).  You should be able to see in the resulting plot the magenta-cyan (or, in more recent versions of `e1071` -- yellow-brown) classification boundary as computed by this model.  Produce the same kinds of plots using 0.5 and 50 as values of `gamma` also.  Compare classification boundaries between these three plots and describe how they are impacted by the change in the value of `gamma`.  Can you trace it back to the role of `gamma` in the equation introducing it with the radial kernel in ISLR?

``` {r subproblem4pt1}
wifiLocDatShrunk <- wifiLocDat[, c(1, 2, 8)]
for ( myGamma in c(0.5, 5, 50)) {
  svmRes <- svm(Loc~WiFi1+WiFi2, data = wifiLocDatShrunk, kernel="radial", cost=10, gamma=myGamma)
  plot(svmRes, data=wifiLocDatShrunk)
}
```

**As the value of gamma increases, the kernel becomes non-zero only for extremely close points. Overall, the model becomes very flexible, resulting in low bias but high variance and potential for overfitting.**

## Sub-problem 4b (10 points): test error for SVM with radial kernel

Similar to how it was done above for support vector classifier (and KNN), set up a resampling process that will repeatedly: a) split the entire dataset (using all attributes as predictors) into training and test datasets, b) use `tune` function to determine optimal values of `cost` and `gamma` and c) calculate test error using these values of `cost` and `gamma`.  Consider what you have learned above about the effects of the parameters `cost` and `gamma` to decide on the starting ranges of their values to be evaluated by `tune`. Additionally, experiment with different sets of their values and discuss in your solution the results of it and how you would go about selecting those ranges starting from scratch.  Present resulting test error graphically, compare it to that of support vector classifier (with linear kernel), random forest and KNN classifiers obtained above and discuss results of these comparisons. 

``` {r problem4pt2}
errorrates <- vector()
for (iTry in 1:10) {
  trainIdx <- sample(nrow(wifiLocDat), nrow(wifiLocDat), replace=TRUE)
  wTrain <- wifiLocDat[trainIdx,]
  wTest <- wifiLocDat[-trainIdx,]
  tune.out=tune(svm,Loc~.,data=wifiLocDat,kernel="linear",
ranges=list(cost=c(0.1, 1, 2, 5, 10, 20, 50,100), gamma=c(0.1, 1, 5, 10, 50)))
  bestmod <- tune.out$best.model
  print(tune.out$best.parameters)
  ypred <- predict(bestmod, wTest)
  resultstable <- table(predict=ypred, truth=wTest$Loc)
  errorrate <- (1-sum(diag(resultstable)/sum(resultstable)))
  errorrates[iTry] <- errorrate
}

plot(errorrates)
```
**The results of the cost and gamma tuning are fairly consistent. Minimizing error, across all 10 tunings, is associated with a cost of 1 or 2 and a gamma value of 0.1. The plot reveals the corresponding error rates to fall between 0.008 and 0.02, fairly similar to the original SVM results. However, it does appear that the tuning of gamma has reduced error minorly, at least on average. It seems that across all 3 models of KNN, random forest, and SVM, our error is minimized at about 0.001. SVM in this case does seem to outperform KNN and random forest, at least slightly, suggesting that the tuning of parameters is highly important in SVM and will reinforce its strength as a classifying strategy.**
